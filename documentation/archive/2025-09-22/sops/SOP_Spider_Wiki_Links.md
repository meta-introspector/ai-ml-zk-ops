# SOP: Spidering Wikipedia Links from Cached Content

## 1. Purpose

This Standard Operating Procedure (SOP) describes the use of the `spider_wiki_links.sh` script to extract new Wikipedia article links from previously cached HTML content. This process helps in discovering related articles and expanding the local knowledge base for LLM context generation or other data analysis tasks.

## 2. Scope

This SOP applies to anyone needing to identify and collect new Wikipedia URLs based on the content of existing locally cached Wikipedia articles.

## 3. Prerequisites

-   A working Bash environment.
-   `grep`, `sed`, `ls`, `cat`, `sort`, `printf` utilities available.
-   `lib_exec.sh` library sourced.
-   A `wikipedia_cache/` directory populated with HTML files of Wikipedia articles (e.g., generated by `cache_wikipedia_sources.sh`).

## 4. Procedure

The `scripts/spider_wiki_links.sh` script automates the extraction of new Wikipedia links.

### 4.1. Script Location

The script is located at: `/data/data/com.termux.nix/files/home/pick-up-nix2/source/github/meta-introspector/streamofrandom/2025/09/22/scripts/spider_wiki_links.sh`

### 4.2. Usage

To run the script, execute it from the project root:

```bash
/data/data/com.termux.nix/files/home/pick-up-nix2/source/github/meta-introspector/streamofrandom/2025/09/22/scripts/spider_wiki_links.sh
```

### 4.3. Script Functionality

The script performs the following steps:

1.  **Sources `lib_exec.sh`**: It includes the `execute_cmd` function for robust command execution.
2.  **Lists Cached HTML Files**: It identifies all `.html` files within the `wikipedia_cache/` directory.
3.  **Extracts Links from Each File**: For every cached HTML file:
    -   It uses `grep -oE 'href="/wiki/[^"#:]+"'` to find all `href` attributes that point to `/wiki/` paths, excluding links with `#` (anchors) or `:` (special pages like `File:` or `Category:`).
    -   `sed` is used to clean up the extracted links, removing `href="` and `"` and prepending `https://en.wikipedia.org/` to form full Wikipedia URLs.
    -   `sort -u` is applied to get unique links from each file.
4.  **Collects Unique New URLs**: All extracted unique links from all processed HTML files are collected into an array.
5.  **Outputs Unique URLs**: Finally, it prints a sorted list of all unique Wikipedia URLs found across all cached articles to standard output.

## 5. Verification

-   After execution, review the standard output to see the list of newly discovered Wikipedia URLs.
-   These URLs can then be manually reviewed and potentially added to the `WIKI_URLS` array in `cache_wikipedia_sources.sh` for further caching.

## 6. Related Files

-   `/data/data/com.termux.nix/files/home/pick-up-nix2/source/github/meta-introspector/streamofrandom/2025/09/22/lib/lib_exec.sh`: Provides the `execute_cmd` function.
-   `/data/data/com.termux.nix/files/home/pick-up-nix2/source/github/meta-introspector/streamofrandom/2025/09/22/scripts/cache_wikipedia_sources.sh`: Script used to populate the `wikipedia_cache/` directory.
-   `wikipedia_cache/`: Directory containing the cached Wikipedia HTML files.
